{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70c5f558-7a3e-4410-90ae-d5eab5bf172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -q boto3 sagemaker huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff2e508a-2dfa-4f86-bef7-02e71ebfd7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# More flexible model loading with fallback options\n",
    "MODEL_DIR = os.environ.get(\"MODEL_PATH\", \"/opt/ml/model\")\n",
    "MODEL_ID = os.environ.get(\"MODEL_ID\", \"gpt2\")  # Default fallback to a HF model\n",
    "\n",
    "# Initialize Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models at startup - using a function to handle errors better\n",
    "def load_model():\n",
    "    try:\n",
    "        # First try to load from local path\n",
    "        print(f\"Attempting to load model from local path: {MODEL_DIR}\")\n",
    "        if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
    "            model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "            print(\"Model loaded successfully from local path\")\n",
    "        else:\n",
    "            # Fallback to downloading from Hugging Face\n",
    "            print(f\"Local model not found. Loading model from Hugging Face: {MODEL_ID}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "            print(f\"Model {MODEL_ID} loaded successfully from Hugging Face\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load model when the script starts\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "@app.route(\"/ping\", methods=[\"GET\"])\n",
    "def ping():\n",
    "    # Health check endpoint required by SageMaker\n",
    "    return jsonify({\"status\": \"healthy\"}), 200\n",
    "\n",
    "@app.route(\"/invocations\", methods=[\"POST\"])\n",
    "def invoke():\n",
    "    try:\n",
    "        # Parse input data from request\n",
    "        data = request.get_json()\n",
    "        input_text = data.get(\"input\", \"\")\n",
    "\n",
    "        print(f\"Received input: {input_text}\")\n",
    "\n",
    "        # Generate response using the model\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        response = tokenizer.decode(outputs[0])\n",
    "\n",
    "        print(f\"Generated response: {response}\")\n",
    "        return jsonify({\"response\": response})\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {str(e)}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This will be used when running the Flask server directly (not through gunicorn)\n",
    "    app.run(host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ee1d20f-2936-48ea-bfcb-3e94e40a797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "torch>=1.10.0\n",
    "transformers>=4.18.0\n",
    "flask>=2.0.0\n",
    "gunicorn>=20.1.0\n",
    "numpy>=1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48bdae1a-8681-460a-89bc-22b2965fe8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM python:3.8\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application files\n",
    "COPY inference.py /app/\n",
    "COPY requirements.txt /app/\n",
    "\n",
    "# Create the model directory that SageMaker expects\n",
    "RUN mkdir -p /opt/ml/model\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Create a serve script that SageMaker expects\n",
    "RUN echo '#!/bin/bash\\ncd /app && gunicorn --bind 0.0.0.0:8080 inference:app' > /usr/local/bin/serve && \\\n",
    "    chmod +x /usr/local/bin/serve\n",
    "\n",
    "# Environment variables\n",
    "ENV MODEL_PATH=\"/opt/ml/model\"\n",
    "ENV MODEL_ID=\"gpt2\"\n",
    "\n",
    "# Make sure serve is in PATH\n",
    "ENV PATH=\"/usr/local/bin:${PATH}\"\n",
    "\n",
    "# Expose the port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Set working directory as per SageMaker requirements\n",
    "WORKDIR /app\n",
    "\n",
    "# Command to run when container starts\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0eeceed-d345-4e64-8714-875cde2055e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311141549115\n",
      "us-east-1\n",
      "llm-autogen-app\n",
      "311141549115.dkr.ecr.us-east-1.amazonaws.com/llm-autogen-app\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "✅ Repository llm-autogen-app already exists.\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/0)  docker:default\n",
      "\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 784B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              0.1s\n",
      "\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (13/13) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 784B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              0.1s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [1/8] FROM docker.io/library/python:3.8@sha256:d411270700143fa2683cc8  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2.53kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/8] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/8] COPY inference.py /app/                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/8] COPY requirements.txt /app/                               0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [5/8] RUN mkdir -p /opt/ml/model                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [6/8] RUN pip install --no-cache-dir -r requirements.txt        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [7/8] RUN echo '#!/bin/bash\\ncd /app && gunicorn --bind 0.0.0.  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [8/8] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:a547cfb05abc06cf5f8cb7c3b7f8fc3b7052b5834cbd3  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/llm-autogen-app                         0.0s\n",
      "The push refers to repository [311141549115.dkr.ecr.us-east-1.amazonaws.com/llm-autogen-app]\n",
      "\n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B9f63c304: Preparing \n",
      "\u001b[1B04b91af8: Preparing \n",
      "\u001b[1Bdd26395d: Preparing \n",
      "\u001b[1B58a7a2f6: Preparing \n",
      "\u001b[1B108e7db9: Preparing \n",
      "\u001b[1Bb4681d05: Preparing \n",
      "\u001b[1B710ca3c7: Preparing \n",
      "\u001b[1Be4d52b5a: Preparing \n",
      "\u001b[1B8afd69b3: Preparing \n",
      "\u001b[1B433c3a29: Preparing \n",
      "\u001b[1Bc7a486d9: Preparing \n",
      "\u001b[1Ba6961052: Preparing \n",
      "latest: digest: sha256:18ec04625192f0757660ca2bb5c0ca1a2f0f16ef0e284b0d64691ee50b23ac2b size: 3252\n",
      "✅ Docker image pushed to: 311141549115.dkr.ecr.us-east-1.amazonaws.com/llm-autogen-app\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import subprocess\n",
    "\n",
    "# Get AWS account ID\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "repository_name = \"llm-autogen-app\"\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(repository_name)\n",
    "\n",
    "# Full ECR URL\n",
    "ecr_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}\"\n",
    "\n",
    "print(ecr_uri)\n",
    "\n",
    "# Login to AWS ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {ecr_uri}\n",
    "\n",
    "# Create the repository if it does not exist\n",
    "ecr_client = boto3.client(\"ecr\", region_name=region)\n",
    "try:\n",
    "    ecr_client.create_repository(repositoryName=repository_name)\n",
    "    print(f\"✅ Created repository: {repository_name}\")\n",
    "except ecr_client.exceptions.RepositoryAlreadyExistsException:\n",
    "    print(f\"✅ Repository {repository_name} already exists.\")\n",
    "\n",
    "# Build, tag, and push the Docker image\n",
    "!docker build -t {repository_name} .\n",
    "!docker tag {repository_name}:latest {ecr_uri}:latest\n",
    "!docker push {ecr_uri}:latest\n",
    "\n",
    "print(f\"✅ Docker image pushed to: {ecr_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05a15769-ce84-4658-9361-b7e20a55d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN     \n",
      "tcp6       0      0 :::8080                 :::*                    LISTEN     \n"
     ]
    }
   ],
   "source": [
    "!netstat -tuln | grep 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a446a57-da9e-4e82-a395-31bfda252487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run -p 8082:8080 311141549115.dkr.ecr.us-east-1.amazonaws.com/llm-autogen-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74754092-8ee2-4922-b3c7-f7beda88be58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/24/25 20:45:42] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: llm-autogen-app-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-02-24-20-45-42-454      <a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/24/25 20:45:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: llm-autogen-app-\u001b[1;36m2025\u001b[0m-02-24-20-45-42-454      \u001b]8;id=111846;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=642028;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/24/25 20:45:43] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name                                     <a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py#5889\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5889</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         llm-autogen-app-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-02-24-20-45-43-050                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/24/25 20:45:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name                                     \u001b]8;id=282183;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=117427;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py#5889\u001b\\\u001b[2m5889\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         llm-autogen-app-\u001b[1;36m2025\u001b[0m-02-24-20-45-43-050                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name llm-autogen-app-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-02-24-20-45-43-050    <a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py#4711\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4711</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name llm-autogen-app-\u001b[1;36m2025\u001b[0m-02-24-20-45-43-050    \u001b]8;id=386745;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=885362;file:///home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py#4711\u001b\\\u001b[2m4711\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------!✅ SageMaker Serverless Endpoint Deployed!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Define the SageMaker Model\n",
    "model = Model(\n",
    "    image_uri=ecr_uri + \":latest\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    ")\n",
    "\n",
    "# Deploy the model as a serverless endpoint\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,\n",
    "    max_concurrency=5\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    serverless_inference_config=serverless_config\n",
    ")\n",
    "\n",
    "print(\"✅ SageMaker Serverless Endpoint Deployed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70c6506b-507b-40ee-9c81-81d9df9dfaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SageMaker endpoint using two different methods...\n",
      "Response using Predictor:\n",
      "{'response': 'What is LLM?\\n\\nLLM is a programming language that is designed to be used in'}\n",
      "Response using boto3:\n",
      "{'response': 'What is LLM?\\n\\nLLM is a programming language that is designed to be used in'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Define your endpoint name\n",
    "endpoint_name = \"llm-autogen-app-2025-02-24-20-45-43-050\"  # Replace with your actual endpoint name\n",
    "\n",
    "# Initialize the SageMaker runtime client\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Method 1: Using the SageMaker Predictor class\n",
    "def test_endpoint_with_predictor():\n",
    "    try:\n",
    "        # Initialize the predictor\n",
    "        predictor = Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sagemaker.Session(),\n",
    "            serializer=JSONSerializer(),\n",
    "            deserializer=JSONDeserializer()\n",
    "        )\n",
    "        \n",
    "        # Prepare input data (as a Python dict, not JSON string)\n",
    "        input_data = {\"input\": \"What is LLM?\"}\n",
    "        \n",
    "        # Make prediction\n",
    "        response = predictor.predict(input_data)  # ✅ Corrected: No json.dumps()\n",
    "        print(\"Response using Predictor:\")\n",
    "        print(response)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Predictor method: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Method 2: Using the boto3 sagemaker-runtime client directly\n",
    "def test_endpoint_with_boto3():\n",
    "    try:\n",
    "        # Prepare input data\n",
    "        input_data = {\"input\": \"What is LLM?\"}\n",
    "        \n",
    "        # Invoke the endpoint\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(input_data)\n",
    "        )\n",
    "        \n",
    "        # Process the response\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        print(\"Response using boto3:\")\n",
    "        print(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error with boto3 method: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Execute both methods to test the endpoint\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing SageMaker endpoint using two different methods...\")\n",
    "    test_endpoint_with_predictor()\n",
    "    test_endpoint_with_boto3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c873ca-06c6-4ce4-9f2e-6fdc65f6064a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
